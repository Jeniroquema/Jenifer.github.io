# Blog

### First Blog
#### Why learning data science in social and interdisciplinary contexts matters
Data science, a technical skill once not as known, now is used to shape many of the decisions that define everyday life. From the content we see online to the policies that affect education, healthcare, and housing, data-driven systems influence how resources are distributed and whose voices are amplified. Data is no longer just about numbers but rather it is about people, communities, and the structures that govern them.

Modeling social topics and people allows us to better understand patterns that are often invisible or overlooked at an individual level. Social data can reveal disparities, trends, and relationships that help explain why certain outcomes persist across time and place. However, people are not static variables. Social behavior is shaped by history, culture, and power, and these factors cannot always be captured in a dataset or in neatly organized excel sheets. When models fail to account for this complexity, they risk simplifying lived experiences or reinforcing existing inequalities.

Data about people carry real consequences, and handling it comes with serious responsibilities. One of the most important is protecting privacy and respecting consent, especially when individuals may not fully understand how their data is collected or used. There is also the responsibility to question bias. Data often reflects the systems that produced it, including structural inequities, and without critical evaluation, models can reproduce those same harms at scale. Ethical data science requires transparency, accountability, and an awareness of who benefits from data-driven decisions while also considering those who do not.

Through this course, I hope to better understand how data science can be practiced with intention and care. Beyond technical skills, I want to learn how to ask meaningful questions, interpret results thoughtfully, and consider the broader social impact of my work. Studying data science in social and interdisciplinary contexts emphasizes that innovation should not only be efficient or predictive, but also responsible, inclusive, and oriented toward the public good.

### Second Blog
#### Ethically ambiguous spaces in Data Modeling
Data modeling is often described as a neutral, technical process, but in practice it frequently operates in ethically ambiguous spaces. Ethically ambiguous data modeling refers to situations where a model may function exactly as intended, yet still risks causing harm due to concerns about fairness and impact. These gray areas occur when models rely on imperfect data shaped by historical inequalities, which means that even careful design choices can reproduce existing harms. In these situations, ethical uncertainty is not caused by broken rules, but from the gap between technical success and social outcomes.

One way data scientists can reduce ethical risk in ambiguous modeling situations is through data auditing. Auditing data means closely examining where data comes from, who it represents, and who it leaves out. Many datasets reflect historical or structural inequalities, and without careful review, models trained on this data can unintentionally reinforce those patterns. By auditing for missing values, biased proxies, or uneven representation across populations, data scientists can better understand the limitations of their models and avoid treating biased data as objective truth.

Another important practice is transparency, which goes hand in hand with documentation. When models are treated as black boxes, even by their users, ethical uncertainty can and will arise. Clear documentation about how data was collected, what assumptions were made, and what the model is and is not designed to do helps prevent misuse. Transparency allows decision makers and stakeholders to interpret model outputs more cautiously and ensures accountability when models influence real world outcomes.

The last one I'd like to point out is ongoing monitoring and impact assessment. This is essential once a model has been deployed. Ethical concerns do not end at deployment, they often emerge over time as models interact with changing social conditions. A model that performs well initially may begin to disadvantage certain groups as patterns shift. Regularly evaluating model outcomes, especially across different populations, helps identify unintended consequences early and allows for adjustments before harm becomes widespread. These three practices emphasize that data modeling is not a one time technical task, but an ongoing process of reflection, evaluation, and care.

An example I can think of is predictive policing models that use arrest data to predict crimes. If the model shows that certain neighborhoods or racial groups have higher arrest counts, it may seem accurate based on historical records. However, these models often fail to account for population size, socio-economic context, or systemic biases in law enforcement. As a result, even a technically correct model can reinforce harmful stereotypes, misrepresent communities, and influence policy in ways that unfairly target marginalized groups. This demonstrates why ongoing monitoring, transparency, and careful auditing are crucial. Numbers alone do not tell the whole story.
